---
bg: "tools.jpg"
layout: post
title:  "A Walkthrough of Convolutional Neural Network"
crawlertitle: "Suki's Blog"
summary: "A Walkthrough of Convolutional Neural Network"
date:   2017-05-31 20:09:47 +0700
categories: posts
tags: ['deep-learning']
author: suki lau
---

Among the diverse deep learning architecture, convolutional neural network (convnets for short) stands out for its unprecedented performance on computer vision. It is artificial neural network inspired by the animal visual cortex and sucessfully applied to vision recognition tasks. 

In this article, I will provide a walkthrough on how to construct a convnet for simple image recognition task, how to tune hyper-parameters for convnet and how to visualize the hidden convolutional layers. 


## Motivation

First things first, what do we want our computer to do?  When we see a cat running in backyard or sleeping on a couch, our minds subconsciously recognize it as a cat in whatever settings. We want our computer to do similar things for us, that is to take an image as input, figure out its unique fetaures and label the image as an output.  This is basically what convnet can do for us. 


## What is convnet?

At its most basic, convnet is a special kind of neural networks which contains at least one convolutional layer. A typical convnet structure includes a convolutional layer which takes the image, nonlinear activation layer, pooling (downsampling) and fully connected layer to output the classification labels.

A convnet differs from a regular neural network by the use of convolutional layer. In a regular neural network, we use the entire image to train the network. It works well for simple centered image (for example a centered handwritten digit image) but fails to recognize image with more complex variation (for example a running cat in backyard). Having more hidden layers to learn abstract features would help but it is rather impractical as we need far too many neurons to train and store in memory.

On the other hand, a convnet recognizes a object by first looking for low level features such as edges, lines and curves, and then building up more abstract features (for example what makes a cat a cat) through series of convolutional layers. During the learning process in the convolutional layer, the network learns individual parts of an object by shared filters (or feature detectors) on small regions of the image, and add them up to build abstract features. The use of shared filters largely reduce the actual parameter learning while still having the capability to build complex model. A convnet also generalizes better for complex image recognition as learned filters can be reused to detect features compositionally through convolutional layers.


## Hyper-parameter tuning

Tuning hyperparameters for deep neural network is difficult as it is slow to train a deep neural network and there are numerours parameters to configure. Here is a quick walkthrough of hyper-parameters for a convnet.

#### Activation function	
Activation funtion introduce non-linearity to the model. Usually, rectifier work wells with convnet. Other alternative are sigmoid, tanh and other activation functions.

#### Number of hidden layers and units
It is usually good to add more layers until the test error no longer improves. The trade off is that it is computationally expensive to train the network.  Having a small amount of units may lead to underfitting while having more units are usually not harmful with appropriate regularization. 

#### Weight initialization
We should initialize the weights with small random numbers to prevent dead neurons, but not too small to avoid zero gradient. Uniform distribution usually works well. There are also other alternatives to choose from.

#### Learning rate
Learning rate controls how much to update the weight in the optimization algorithm.  We can use fixed learning rate, gradually decreasing learning rate, momentum based methods to change the learning rate or adaptive learning rates depending on our choice of optimizer such as SGD, Adam, Adagrad, AdaDelta, RMSProp.

#### Convolutional filter size	5x5

#### Number of epochs

#### Batch size	64

* no. of epochs	20

#### Dropout for regularization
Dropout is a preferable regularization technique to avoid overfitting in deep neural networks. It drops out units in neural network according to the desired probability. A default value of 0.5 is a good choice in general.

#### Grid search, randomized search or others?
This makes GridSearch difficult for deep neural nets.
To implement GridSearch efficiently, it is better to start with coarse grids of individual hyperparameters on a smaller dataset, followed by fine grids on combinations of selected hyperparameters.
Further Thoughts

Implementation of GridSearch could go crazy for deep neural nets. Randomized search and Bayesian optimization could be more effective in hyperparameter tuning for convnet. 

Random Search has been found to be way more efficient compared to Grid Search.







## Visualization







#### References:
* [Understanding Convolutions](http://colah.github.io/posts/2014-07-Understanding-Convolutions/)
* [Convolutional Neural Networks: Architectures, Convolution / Pooling Layers](http://cs231n.github.io/convolutional-networks/)
* [Understanding and Visualizing Convolutional Neural Networks](http://cs231n.github.io/understanding-cnn/)
* [Convolutional Neural Networks (CNNs): An Illustrated Explanation](http://xrds.acm.org/blog/2016/06/convolutional-neural-networks-cnns-illustrated-explanation/)
* [Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)

* [](http://rishy.github.io/ml/2017/01/05/how-to-train-your-dnn/)

* [](https://ml4a.github.io/ml4a/convnets/)
* [Understanding and Visualizing Convolutional Neural Networks](http://cs231n.github.io/understanding-cnn/]
* [](https://medium.com/rants-on-machine-learning/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881)
* [](https://ml4a.github.io/ml4a/convnets/)
* [](https://ml4a.github.io/ml4a/looking_inside_neural_nets/)
