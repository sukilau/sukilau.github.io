---
bg: "tools.jpg"
layout: post
title:  "A Walkthrough of Convolutional Neural Network"
crawlertitle: "Suki's Blog"
summary: "A Walkthrough of Convolutional Neural Network"
date:   2017-05-31 20:09:47 +0700
categories: posts
tags: ['deep-learning']
author: suki lau
---

Among the diverse deep learning architecture, convolutional neural network (convnets for short) stands out for its unprecedented performance on computer vision. It is an artificial neural network inspired by the animal visual cortex and has been sucessfully applied to vision recognition tasks. 

In this article, I will provide a walkthrough on convnet, how to tune hyperparameters and how to visualize the hidden convolutional layers. 


## Motivation

First things first, what do we want our computer to do?  When we see a cat running in backyard or sleeping on a couch, our minds subconsciously recognize it as a cat. We want our computer to do similar things for us, that is to take an image as input, figure out its unique fetaures and label the image as an output.  This is basically what convnet can do for us. 


## What is convnet?

At its most basic, convnet is a special kind of neural networks which contains at least one convolutional layer. A typical convnet structure includes a convolutional layer which takes the image, nonlinear activation layer, pooling (downsampling) and fully connected layer to output the classification labels.

A convnet differs from a regular neural network by the use of convolutional layer. In a regular neural network, we use the entire image to train the network. It works well for simple centered image (for example a centered handwritten digit image) but fails to recognize image with more complex variation (for example a running cat in backyard). Having more hidden layers to learn abstract features would help but it is rather impractical as we need far too many neurons to train and store in memory.

On the other hand, a convnet recognizes a object by first looking for low level features such as edges, lines and curves, and then building up more abstract features (for example what makes a cat a cat) through series of convolutional layers. During the learning process in the convolutional layer, the network learns individual parts of an object by shared filters (or feature detectors) on small regions of the image, and add them up to build abstract features. The use of shared filters largely reduce the actual parameter learning. A convnet also generalizes better for complex image recognition as learned filters can be reused to detect abstract features compositionally through convolutional layers.


## Hyperparameter tuning

Tuning hyperparameters for deep neural network is difficult as it is slow to train a deep neural network and there are numerours parameters to configure. In this part, we briefly survey the hyperparameters for convnet.

#### Learning rate
Learning rate controls how much to update the weight in the optimization algorithm.  We can use fixed learning rate, gradually decreasing learning rate, momentum based methods to change the learning rate or adaptive learning rates, depending on our choice of optimizer such as SGD, Adam, Adagrad, AdaDelta, RMSProp.

#### Number of epochs
Number of epochs is the the number of times the entire training set pass through the neural network. We should increase the number of epochs until we see a small gap between the test error and the training error.

#### Batch size	
Mini-batch is usually preferable in the learning process of convnet. A range of 16 to 128 is a good choice to test with. We should note that convnet is sensitive to batch size.

#### Activation function
Activation funtion introduce non-linearity to the model. Usually, rectifier works well with convnet. Other alternative are sigmoid, tanh and other activation functions depening on the task.

#### Number of hidden layers and units
It is usually good to add more layers until the test error no longer improves. The trade off is that it is computationally expensive to train the network.  Having a small amount of units may lead to underfitting while having more units are usually not harmful with appropriate regularization. 

#### Weight initialization
We should initialize the weights with small random numbers to prevent dead neurons, but not too small to avoid zero gradient. Uniform distribution usually works well.

#### Dropout for regularization
Dropout is a preferable regularization technique to avoid overfitting in deep neural networks. The method simply drops out units in neural network according to the desired probability. A default value of 0.5 is a good choice to test with.

#### Grid search or randomized search

Manually tuning hyperparameter is painful and also impractical. There are two generic approaches to sampling search candidates. Grid search exhaustively search all parameter combinations for given values. Random search sample a given number of candidates from a parameter space with a specified distribution. 

To implement grid search more efficiently, it is better to start with coarse ranges of hyperparameter values in the initial stage. It is also helpful to perform coarse grid search for smaller number of epochs or smaller training set.  The next stage would then perform a narrow search with more epochs or entire training set. 

Although grid search is useful in many machine learning algoirthms, it is not efficient in tuning hyperparamter for deep neural network as the number of parameters increases, computation grows exponentially. Random search has been found more efficient compared to grid search in hyperparameter tuning for deep neural netwlork (see [Paper on "Random Search for Hyper-Parameter Optimization"](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)).  It is also helpful to combine with some manual tuning on hyperparameters based on prior experience.


## Visualization

We can better understand how convnet learns features if we can visualize the convolutional layers. Two straight forward ways are to visualize the activations and the weights. The activations usually look more sparse and localized as training progresses. If the activations have dark pixels for many different inputs, it may indicate dead filters (zero activation map) due to high learning rates.

Well-trained networks usually have nice and smooth filters without any noisy patterns. Noisy patterns observed in the weights may indicate that network hasn't been trained long enough, or low regularization strength that may have led to overfitting.


#### References:

* [Deep Learning](http://www.deeplearningbook.org/)
* [Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)
* [Convolutional Neural Networks: Architectures, Convolution / Pooling Layers](http://cs231n.github.io/convolutional-networks/)
* [Understanding and Visualizing Convolutional Neural Networks](http://cs231n.github.io/understanding-cnn/)
* [Understanding Neural Networks Through Deep Visualization](http://yosinski.com/deepvis)
* [Understanding Convolutions](http://colah.github.io/posts/2014-07-Understanding-Convolutions/)
* [How to Grid Search Hyperparameters for Deep Learning Models in Python With Keras](http://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/)
